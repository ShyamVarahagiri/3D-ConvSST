{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6472,"status":"ok","timestamp":1700579568785,"user":{"displayName":"Shyam","userId":"04546190256590993427"},"user_tz":-330},"id":"-Ew3M3xisBj6"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as dataf\n","import scipy.io as io\n","import time\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.patheffects as PathEffects\n","import seaborn as sns\n","from sklearn import manifold, datasets\n","from sklearn.metrics.pairwise import pairwise_distances\n","from scipy.spatial.distance import squareform\n","import torch.nn as nn\n","from sklearn.manifold import TSNE\n","import auxil\n","from torchsummary import summary"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1700579568785,"user":{"displayName":"Shyam","userId":"04546190256590993427"},"user_tz":-330},"id":"uVETAQ5cH7DU"},"outputs":[],"source":["class HetConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,padding = None, bias = None,p = 64, g = 64):\n","        super(HetConv, self).__init__()\n","        # Groupwise Convolution\n","        self.gwc = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,groups=g,padding = kernel_size//3, stride = stride)\n","        # Pointwise Convolution\n","        self.pwc = nn.Conv2d(in_channels, out_channels, kernel_size=1,groups=p, stride = stride)\n","#         self.pwc = eSEModule(in_channels,bias=True,stride = 1 )\n","    def forward(self, x):\n","        return self.gwc(x) + self.pwc(x)\n","\n","\n","\n","class Attention(nn.Module):\n","    \"\"\"Attention mechanism.\n","\n","    Parameters\n","    ----------\n","    dim : int\n","        The input and out dimension of per token features.\n","\n","    n_heads : int\n","        Number of attention heads.\n","\n","    qkv_bias : bool\n","        If True then we include bias to the query, key and value projections.\n","\n","    attn_p : float\n","        Dropout probability applied to the query, key and value tensors.\n","\n","    proj_p : float\n","        Dropout probability applied to the output tensor.\n","\n","\n","    Attributes\n","    ----------\n","    scale : float\n","        Normalizing consant for the dot product.\n","\n","    qkv : nn.Linear\n","        Linear projection for the query, key and value.\n","\n","    proj : nn.Linear\n","        Linear mapping that takes in the concatenated output of all attention\n","        heads and maps it into a new space.\n","\n","    attn_drop, proj_drop : nn.Dropout\n","        Dropout layers.\n","    \"\"\"\n","    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n","        super().__init__()\n","        self.n_heads = n_heads\n","        self.dim = dim\n","        self.head_dim = dim // n_heads\n","        self.scale = self.head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_p)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_p)\n","\n","    def forward(self, x):\n","        \"\"\"Run forward pass.\n","\n","        Parameters\n","        ----------\n","        x : torch.Tensor\n","            Shape `(n_samples, n_patches + 1, dim)`.\n","\n","        Returns\n","        -------\n","        torch.Tensor\n","            Shape `(n_samples, n_patches + 1, dim)`.\n","        \"\"\"\n","        n_samples, n_tokens, dim = x.shape\n","\n","        if dim != self.dim:\n","            raise ValueError\n","\n","        qkv = self.qkv(x)  # (samples, n_patches, 3 * dim)\n","        qkv = qkv.reshape(\n","                n_samples, n_tokens, 3, self.n_heads, self.head_dim\n","        )  # (samples, n_patches, 3, n_heads, head_dim)\n","        qkv = qkv.permute(\n","                2, 0, 3, 1, 4\n","        )  # (3, n_samples, n_heads, n_patches, head_dim)\n","\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","        k_t = k.transpose(-2, -1)  # (n_samples, n_heads, head_dim, n_patches)\n","        dp = (\n","           q @ k_t\n","        ) * self.scale # (n_samples, n_heads, n_patches, n_patches)\n","        attn = dp.softmax(dim=-1)  # (n_samples, n_heads, n_patches, n_patches)\n","        attn = self.attn_drop(attn)\n","\n","        weighted_avg = attn @ v  # (n_samples, n_heads, n_patches, head_dim)\n","        weighted_avg = weighted_avg.transpose(\n","                1, 2\n","        )  # (n_samples, n_patches + 1, n_heads, head_dim)\n","        weighted_avg = weighted_avg.flatten(2)  # (n_samples, n_patches, dim)\n","\n","        x = self.proj(weighted_avg)  # (n_samples, n_patches, dim)\n","        x = self.proj_drop(x)  # (n_samples, n_patches, dim)\n","\n","        return x\n","\n","\n","class MLP(nn.Module):\n","    \"\"\"Multilayer perceptron.\n","\n","    Parameters\n","    ----------\n","    in_features : int\n","        Number of input features.\n","\n","    hidden_features : int\n","        Number of nodes in the hidden layer.\n","\n","    out_features : int\n","        Number of output features.\n","\n","    p : float\n","        Dropout probability.\n","\n","    Attributes\n","    ----------\n","    fc : nn.Linear\n","        The First linear layer.\n","\n","    act : nn.GELU\n","        GELU activation function.\n","\n","    fc2 : nn.Linear\n","        The second linear layer.\n","\n","    drop : nn.Dropout\n","        Dropout layer.\n","    \"\"\"\n","    def __init__(self, in_features, hidden_features, out_features, p=0.):\n","        super().__init__()\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = nn.GELU()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(p)\n","\n","    def forward(self, x):\n","        \"\"\"Run forward pass.\n","\n","        Parameters\n","        ----------\n","        x : torch.Tensor\n","            Shape `(n_samples, n_patches + 1, in_features)`.\n","\n","        Returns\n","        -------\n","        torch.Tensor\n","            Shape `(n_samples, n_patches +1, out_features)`\n","        \"\"\"\n","        x = self.fc1(\n","                x\n","        ) # (n_samples, n_patches, hidden_features)\n","        x = self.act(x)  # (n_samples, n_patches, hidden_features)\n","        x = self.drop(x)  # (n_samples, n_patches, hidden_features)\n","        x = self.fc2(x)  # (n_samples, n_patches, out_features)\n","        x = self.drop(x)  # (n_samples, n_patches, out_features)\n","\n","        return x\n","\n","\n","class Block(nn.Module):\n","    \"\"\"Transformer block.\n","\n","    Parameters\n","    ----------\n","    dim : int\n","        Embeddinig dimension.\n","\n","    n_heads : int\n","        Number of attention heads.\n","\n","    mlp_ratio : float\n","        Determines the hidden dimension size of the `MLP` module with respect\n","        to `dim`.\n","\n","    qkv_bias : bool\n","        If True then we include bias to the query, key and value projections.\n","\n","    p, attn_p : float\n","        Dropout probability.\n","\n","    Attributes\n","    ----------\n","    norm1, norm2 : LayerNorm\n","        Layer normalization.\n","\n","    attn : Attention\n","        Attention module.\n","\n","    mlp : MLP\n","        MLP module.\n","    \"\"\"\n","    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n","        super().__init__()\n","        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n","        self.attn = Attention(\n","                dim,\n","                n_heads=n_heads,\n","                qkv_bias=qkv_bias,\n","                attn_p=attn_p,\n","                proj_p=p\n","        )\n","        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n","        hidden_features = int(dim * mlp_ratio)\n","        self.mlp = MLP(\n","                in_features=dim,\n","                hidden_features=hidden_features,\n","                out_features=dim,\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"Run forward pass.\n","\n","        Parameters\n","        ----------\n","        x : torch.Tensor\n","            Shape `(n_samples, n_patches + 1, dim)`.\n","\n","        Returns\n","        -------\n","        torch.Tensor\n","            Shape `(n_samples, n_patches + 1, dim)`.\n","        \"\"\"\n","        x = x + self.attn(self.norm1(x))\n","        x = x + self.mlp(self.norm2(x))\n","\n","        return x\n","\n","\n","class VisionTransformer(nn.Module):\n","    \"\"\"Simplified implementation of the Vision transformer.\n","\n","    Parameters\n","    ----------\n","\n","    in_chans : int\n","        Number of input channels.\n","\n","    n_classes : int\n","        Number of classes.\n","\n","    embed_dim : int\n","        Dimensionality of the token/patch embeddings.\n","\n","    depth : int\n","        Number of blocks.\n","\n","    n_heads : int\n","        Number of attention heads.\n","\n","    mlp_ratio : float\n","        Determines the hidden dimension of the `MLP` module.\n","\n","    qkv_bias : bool\n","        If True then we include bias to the query, key and value projections.\n","\n","    p, attn_p : float\n","        Dropout probability.\n","\n","    Attributes\n","    ----------\n","    patch_embed : PatchEmbed\n","        Instance of `PatchEmbed` layer.\n","\n","    cls_token : nn.Parameter\n","        Learnable parameter that will represent the first token in the sequence.\n","        It has `embed_dim` elements.\n","\n","    pos_emb : nn.Parameter\n","        Positional embedding of the cls token + all the patches.\n","        It has `(n_patches + 1) * embed_dim` elements.\n","\n","    pos_drop : nn.Dropout\n","        Dropout layer.\n","\n","    blocks : nn.ModuleList\n","        List of `Block` modules.\n","\n","    norm : nn.LayerNorm\n","        Layer normalization.\n","    \"\"\"\n","    def __init__(\n","            self,\n","            in_chans=3,\n","            n_classes=1000,\n","            embed_dim=768,\n","            depth=8,\n","            n_heads=12,\n","            mlp_ratio=4.,\n","            qkv_bias=True,\n","            p=0.,\n","            attn_p=0.,\n","    ):\n","        super().__init__()\n","\n","        self.conv5 = nn.Sequential(\n","            nn.Conv3d(1, 8, (9, 3, 3), padding=(0,1,1), stride = 1),\n","            nn.BatchNorm3d(8),\n","            nn.ReLU()\n","        )\n","        self.conv6 = nn.Sequential(\n","            HetConv(8 * (in_chans - 8), 64, p=1, g = 8),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU()\n","        )\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n","        self.pos_embed = nn.Parameter(\n","                torch.zeros(1, 121, embed_dim)\n","        )\n","        self.pos_drop = nn.Dropout(p=p)\n","        self.blocks = nn.ModuleList(\n","            [\n","                Block(\n","                    dim=embed_dim,\n","                    n_heads=n_heads,\n","                    mlp_ratio=mlp_ratio,\n","                    qkv_bias=qkv_bias,\n","                    p=p,\n","                    attn_p=attn_p,\n","                )\n","                for _ in range(depth)\n","            ]\n","        )\n","        self.convBlock = nn.Conv3d(64, 64, (2, 3, 3), padding=(0,1,1), stride = 1)\n","        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n","        self.pool = nn.AvgPool1d(121)\n","        self.head = nn.Linear(embed_dim, n_classes)\n","\n","\n","    def forward(self, x):\n","        \"\"\"Run the forward pass.\n","\n","        Parameters\n","        ----------\n","        x : torch.Tensor\n","            Shape `(n_samples, in_chans, img_size, img_size)`.\n","\n","        Returns\n","        -------\n","        logits : torch.Tensor\n","            Logits over all the classes - `(n_samples, n_classes)`.\n","        \"\"\"\n","        n_samples = x.shape[0]\n","        x = x.unsqueeze(1)\n","        x = self.conv5(x)\n","        x = x.reshape(x.shape[0],-1,11,11)\n","        x = self.conv6(x) # (samples, channels, h, w)\n","        x = x.reshape(x.shape[0], x.shape[1], -1) # (samples, embedDim, patches)\n","        x = x.transpose(1,2) # (samples, patches, embedDim)\n","        x = x + self.pos_embed  # (n_samples, n_patches, embed_dim)\n","        x = self.pos_drop(x)\n","\n","        for block in self.blocks:\n","            temp = x.transpose(1, 2) # (samples, embedDim, patches)\n","            xNew = block(x) # (samples, patches, embedDim)\n","            xNew = xNew.transpose(1,2) # (samples, embedDim, patches)\n","            temp = temp.unsqueeze(2) # (samples, embedDim, 1, patches)\n","            temp = temp.reshape(temp.shape[0], temp.shape[1], 1, 11, 11) # (samples, embedDim, 1, 11, 11)\n","            xNew = xNew.unsqueeze(2) # (samples, embedDim, 1, patches)\n","            xNew = xNew.reshape(xNew.shape[0], xNew.shape[1], 1, 11, 11) # (samples, embedDim, 1, 11, 11)\n","            x = torch.cat((temp, xNew), dim=2) # (samples, embedDim, 2, 11, 11)\n","            x = self.convBlock(x) # (samples, embedDim, 1, 11, 11)\n","            x = x.reshape(x.shape[0], x.shape[1], 11, 11) # (samples, embedDim, 11, 11)\n","            x = x.reshape(x.shape[0], x.shape[1], -1) # (samples, embedDim, patches)\n","            x = x.transpose(1, 2) # (samples, patches, embedDim)\n","\n","        x = self.norm(x) # (samples, patches, embedDim)\n","        x = x.transpose(1,2)\n","        x = self.pool(x)[:, :, 0] # (samples, embedDim)\n","        x = self.head(x)\n","\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7EtJeYS7ic0I"},"outputs":[],"source":["import os\n","import torch\n","import torch.utils.data as dataf\n","import scipy.io as io\n","import time\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.patheffects as PathEffects\n","import seaborn as sns\n","from sklearn import manifold, datasets\n","from sklearn.metrics.pairwise import pairwise_distances\n","from scipy.spatial.distance import squareform\n","import torch.nn as nn\n","from sklearn.manifold import TSNE\n","import auxil\n","from torchsummary import summary\n","\n","datasetNames = [\"houston\", \"MUUFL\", \"botswana\"]\n","testSizeNumber = 100\n","batchsize = 64\n","EPOCH = 500\n","LR = 4.95e-4\n","HSIOnly = True\n","FileName = \"3dConvSST\"\n","\n","\n","def set_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","\n","\n","for datasetName in datasetNames:\n","  print(\"----------------------------------Training for \",datasetName,\" ---------------------------------------------\")\n","  try:\n","      os.makedirs(datasetName)\n","  except FileExistsError:\n","      pass\n","\n","  data1Name = datasetName\n","\n","  X = io.loadmat('./HSI_Dataset/'+data1Name+'Tr.mat')['HSI_tr']\n","  Y = io.loadmat('./HSI_Dataset/'+data1Name+'Tr.mat')['Label_tr']\n","  NC = X.shape[3]\n","  X_Test = io.loadmat('./HSI_Dataset/'+data1Name+'Te.mat')['HSI_te']\n","  Y_Test = io.loadmat('./HSI_Dataset/'+data1Name+'Te.mat')['Label_te']\n","\n","\n","  TrainPatch1 = torch.from_numpy(X).to(torch.float32)\n","  TrainPatch1 = TrainPatch1.permute(0,3,1,2).to(torch.float32)\n","  TrainLabel1 = torch.from_numpy(Y)\n","  TrainLabel1 = TrainLabel1.long()\n","  TrainLabel1 = TrainLabel1.reshape(-1)\n","\n","  TestPatch1 = torch.from_numpy(X_Test).to(torch.float32)\n","  TestPatch1 = TestPatch1.permute(0,3,1,2).to(torch.float32)\n","  TestLabel1 = torch.from_numpy(Y_Test)\n","  TestLabel1 = TestLabel1.long()\n","  TestLabel1 = TestLabel1.reshape(-1)\n","\n","  Classes = len(np.unique(TrainLabel1))\n","  dataset = dataf.TensorDataset(TrainPatch1,TrainLabel1)\n","  if data1Name in ['Berlin']:\n","      train_loader = dataf.DataLoader(dataset, batch_size=batchsize, shuffle=True, num_workers= 0)\n","  else:\n","      train_loader = dataf.DataLoader(dataset, batch_size=batchsize, shuffle=True, num_workers= 4)\n","  print(\"HSI Train data shape = \", TrainPatch1.shape)\n","  print(\"Train label shape = \", TrainLabel1.shape)\n","\n","  print(\"HSI Test data shape = \", TestPatch1.shape)\n","  print(\"Test label shape = \", TestLabel1.shape)\n","\n","  print(\"Number of Classes = \", Classes)\n","\n","\n","\n","  KAPPA = []\n","  OA = []\n","  AA = []\n","  ELEMENT_ACC = np.zeros((1, Classes))\n","\n","  set_seed(42)\n","  for iterNum in range(1):\n","      cnn = VisionTransformer(in_chans=NC, n_classes=Classes, embed_dim=64, depth=2, n_heads=8).cuda()\n","      summary(cnn, (NC, 11, 11), device='cuda')\n","      optimizer = torch.optim.Adam(cnn.parameters(), lr=LR,weight_decay=5e-3)\n","      loss_func = nn.CrossEntropyLoss()\n","      scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.9)\n","      BestAcc = 0\n","      torch.cuda.synchronize()\n","      # train and test the designed model\n","      for epoch in range(EPOCH):\n","          for step, (b_x1, b_y) in enumerate(train_loader):\n","              # move train data to GPU\n","              b_x1 = b_x1.cuda()\n","              b_y = b_y.cuda()\n","              out1 = cnn(b_x1)\n","              loss = loss_func(out1, b_y)\n","\n","\n","              optimizer.zero_grad()  # clear gradients for this training step\n","              loss.backward()  # backpropagation, compute gradients\n","              optimizer.step()  # apply gradients\n","\n","              if step % 10 == 0:\n","                  cnn.eval()\n","                  pred_y = np.empty((len(TestLabel1)), dtype='float32')\n","                  number = len(TestLabel1) // testSizeNumber\n","                  for i in range(number):\n","                      temp = TestPatch1[i * testSizeNumber:(i + 1) * testSizeNumber, :, :]\n","                      temp = temp.cuda()\n","\n","\n","                      temp2 = cnn(temp)\n","                      temp3 = torch.max(temp2, 1)[1].squeeze()\n","                      pred_y[i * testSizeNumber:(i + 1) * testSizeNumber] = temp3.cpu()\n","                      del temp, temp2, temp3\n","\n","\n","                  if (i + 1) * testSizeNumber < len(TestLabel1):\n","                      temp = TestPatch1[(i + 1) * testSizeNumber:len(TestLabel1), :, :]\n","                      temp = temp.cuda()\n","                      temp2 = cnn(temp)\n","                      temp3 = torch.max(temp2, 1)[1].squeeze()\n","                      pred_y[(i + 1) * testSizeNumber:len(TestLabel1)] = temp3.cpu()\n","                      del temp, temp2, temp3\n","\n","                  pred_y = torch.from_numpy(pred_y).long()\n","                  accuracy = torch.sum(pred_y == TestLabel1).type(torch.FloatTensor) / TestLabel1.size(0)\n","\n","                  print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.cpu().numpy(), '| test accuracy: %.4f' % (accuracy*100))\n","\n","                  # save the parameters in network\n","                  if accuracy > BestAcc:\n","                      BestAcc = accuracy\n","                      torch.save(cnn.state_dict(), datasetName+'/weights.pkl')\n","  #                                   print(\"Weights = \", w0.cpu().detach().numpy())\n","  #                                   print(w1.cpu().detach().numpy())\n","\n","\n","                  cnn.train()\n","          scheduler.step()\n","      torch.cuda.synchronize()\n","\n","\n","      cnn.load_state_dict(torch.load(datasetName+'/weights.pkl'))\n","\n","\n","      cnn.eval()\n","      confusion, oa, each_acc, aa, kappa = auxil.reports(TestPatch1, TestLabel1, datasetName, cnn, testSizeNumber)\n","      print(\"OA AA, Kappa ACCclass\", oa, aa, kappa, each_acc)\n","\n","\n","  cnn.eval()\n","  number = len(TestLabel1)//testSizeNumber\n","  features = []\n","  # pred_y = []\n","  labels = pred_y.data.cpu().numpy()\n","  # print(pred_y)\n","  # print(len(pred_y), np.min(pred_y), np.max(pred_y))\n","  for i in range(number):\n","      temp = TestPatch1[i * testSizeNumber:(i + 1) * testSizeNumber]\n","      temp = temp.cuda()\n","      temp2 = cnn(temp)\n","      fea = temp2\n","      features.append(fea.cpu().detach().numpy())\n","      del temp, temp2\n","\n","  if (i + 1) * testSizeNumber < len(TestLabel1):\n","      temp = TestPatch1[(i + 1) * testSizeNumber:len(TestLabel1)]\n","      temp = temp.cuda()\n","      temp2 = cnn(temp)\n","      fea = temp2\n","      features.append(fea.cpu().detach().numpy())\n","      del temp, temp2\n","\n","\n","  features = np.concatenate(features)\n","  print(\"features = \", features.shape)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1Rk78A0Gjedpz-Suuw8SrmwtT5UdIQsSA","timestamp":1696990771923},{"file_id":"1UcXOGFhbNxMQuikui4SzqJqgG7dbwWdd","timestamp":1693325872121}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
